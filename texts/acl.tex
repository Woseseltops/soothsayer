\documentclass[11pt]{article}
\usepackage{acl2013}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}

\usepackage{covington}
\usepackage{graphicx}
\usepackage{natbib}

\title{Using idiolects and sociolects for autocompletion}
\author{Wessel Stoop (\& Antal van den Bosch?)}

\date{}

\begin{document}
\maketitle

\begin{abstract}

\end{abstract}

\section{Introduction}
Predicting what somebody is going to say entails more than just knowledge of the language he/she uses; it is impossible to say what the next sentence of this text is going to be only on the basis of knowledge of English. [x] show this also true for autocompletion systems: [...]. [y] takes another approach to solve the same problem: he makes the system learn 'on the fly' by adding a recency buffer. This means that a list of the n most recent words is kept in memory. Whenever the word the user is keying in matches a word in this buffer, this word is predicted. So in example \ref{swim}, 'they' will be predicted because it was used recently.

\begin{examples} \label{swim}
\item They decided to go swimming because t ...
\end{examples}

Both approaches significantly increase the number of keystrokes saved, but both approaches also have downsides: for the system by [x] training texts in the same genre are needed, which might not be available, whereas the system by [y] is context-insensitive and thus throws away a lot of intelligence. For example, while a context-sensitive autocompletion system will probably be able to predict 'to' for sentence \ref{tommy}, the one with the recency buffer will predict 'Tommy'.

\begin{examples} 
\item Tommy was going t ... \label{tommy}
\end{examples}

In this paper, a new approach is suggested: using texts written by the user earlier as training material. This way, context-sensitivity can be combined with material that is very likely to be available.

Using an idiolect (i.e. the language from only one person) instead of a general language model makes sense from a linguistic point of view as well:

To investigate the usefulness of idiolects in autocompletion, we will use tweets. Twitter provides an excellent data source for researching idiolects, as Twitter in essence is a collection of language ordered by author. However, when trying to predict new tweets on the basis of earlier tweets, we have a practical and a theoretical problem. The practical problem is that a tweet only consists of 140 characters at most, which means that we only have an acceptable amount of data for the most active Twitter users. The theoretical problem is that the output of a particular person is only part of the idiolect; of course, people know much 'more language' than what they say, otherwise they would not be able to understand people who speak or write differently than how they speak and write themselves. To really 'capture' an idiolect, it might make more sense to use the linguistic \emph{input} for a particular individual, instead of his/her output.

We will try to solve both problems at once by also including the tweets of the 'friends' of the Twitter users investigated in the experiments described here. Since it is not possible to extract an indication of the exact social relation between Twitter users, we will assume that all users who talk to eachother at least 6 times (3 times from A to B and 3 times from B to A) are friends. This approach provides more training material, contributing to a solution for problem 1, and gives us language the investigated users are very likely to have read, contributing to a solution for problem 2. Of all input that could have been chosen for a particular Twitter user, we believe that the language of his/her friends is one of the best choices, as it is well known in the field of sociolinguistics that people that are in some way related try to speak alike [[refs?]]. The resulting language varieties are called sociolects.

\section{Methodology}

\subsection{Autocompletion system: Soothsayer}

The software used for the autocompletion experiments, which has the working title \emph{Soothsayer}, consists of two types of modules: context-sensitive ones and context-insensitive ones. 

\textbf{Context-sensitive modules} are a wrapper around \emph{TiMBL}. TiMBL is an open source software package implementing several memory-based learning algorithms ([[ref]]). For reasons of speed, TiMBL's IGTree was used. IGTree is an approximation of IB1-IG (which is k-nearest neighbour classification), but reduces the classification process to a much faster decision tree. We refer to [[ref]] for details about IGTree's exact workings.

Autocompletion is thus treated by these modules as a classification task. First, Soothsayer transforms the training texts into 4-grams. The first three words of each 4-gram are the features, the last one is the class. On the basis of these 4-grams an IGTree is created, henceforth reffered to as the 'language model'. With this language model, new examples can be classified. Classifying a new example means that a 3-gram is given to TiMBL, and that TiMBL responds with a list of words that can possibly follow this 3-gram, along with confidence values. 

Soothsayer's job thus is collecting three words to the left of what the user is typing, and asking TiMBL to classify these three words. The resulting class, which is the word that will most likely follow what has been keyed in so far (at least, on the basis of the language model that is being used), is showed to the user. If TiMBL returns multiple possible words (which is almost always the case), the word with the highest confidence value is chosen. When the user has already begun keying in the first letters of the next word, only words that start with exactly these letters are taken into consideration. For example, for both sentence \ref{ilike} and \ref{ilikes} Soothsayer will ask TiMBL to classify the 3-gream \emph{'\_ I like'}.

\begin{examples}
\item I like \label{ilike}
\item I like s \label{ilikes}
\end{examples}

Imagine that TiMBL returns the classes 'soup', 'icecream' and 'sandwiches', with a confidence of 0.5, 0.9 and 0.8 respectively. For sentence \ref{ilike}, 'icecream' will be chosen, because it has the highest conficen value. For \ref{ilikes}, however, the 's' has already been given, which rules out 'icecream'. From the words that are still possible 'sandwich' has the highest confidence value, so that is the one that Soothsayer will show the user.
%Confidence uitleggen

\textbf{Context-insensitive} modules work with a frequency list based on the training material. Soothsayer looks at the word that is being keyed in, and returns the most frequent word that matches with what has been given so far. For sentence \ref{ilike}, this will be the most frequent word on the list (because nothing has been given so far), for \ref{ilikes} this will be the most frequent word that starts with an s.

Soothsayer's modules can be concatenated. This means that a second modules takes over once the first module no longer has suggestions, the third module takes over once the second no longer has suggestions, etc. For all experiments described here, only two modules have been used: a context-sensitive one followed by a context-insensitve one, both based on the same material.

\subsection{Training and test material: tweets}

By communicating with the Twitter API we were able to follow a manually created set of Dutch Twitter users from January until April 2013, and save every tweet they produced. Retweets were excluded.
%Meer tweets

To capture the networks of these users as good as possible, we extended our set every 30 minutes with the Twitter user most of the tweets in our collection referred to that was not yet on our list. This way we were able to add Twitter users that were somehow connected to a lot of the Twitter users we were already following. To limit our dataset to Dutch Twitter users, only references in Dutch were counted. Because early tests showed this process often produces famous Twitter users (i.e. people the original users do not know in real life), we also extended our list with the Twitter user that was most referred to in general every 30 minutes. This way, we were able to add the Twitter users that the Twitter users we already were following communicated with a lot. Again, only references in Dutch were counted.

In terms of amount of Tweets, the Twitter users show a Zipfian curve [[ref]]  [[uitzoeken of dat echt zo is!]]; most of the Twitter users we followed only produced a handful of tweets. For that reason, we limited ourselves to the 50 most frequent tweeters in the experiments.

A Twitter user was considered a 'friend' of another Twitter user when both users tweeted to the other person at least three times. We have chosen this number because a total of six tweets from both sides makes it likely that both Twitter users know eachother. So to find out who are the friends of user X, we make a list of which other users he/she addresses at least three times. For each addressee, we count if he/she addresses user X at least three times as well.

As a control corpus, a random selection of the blogs, emails and Wikipedia articles from the SoNaR corpus for written Dutch [[ref]] were used. These texts were chosen because they were believed to be neither very formal nor very informal.

%Hoeveel woorden twitter users? 

\subsection{Evaluation measures}
The prestations of our autocompletion system were measured in 'percentage of keystrokes saved'. We calculate these in two different ways:
\begin{itemize}
\item By only showing the user one prediction at a time, which could be accepted with the spacebar or punctuation, and could be rejected with the tab key. Following [y], 'we do not consider ranked n-best lists of completion suggestions, as in many devices and circumstances it is inefficient or impossible to present these suggestion lists. Inspecting a list of suggestions also poses a larger cognitive load than checking a single suggestion [...].' (ref). The percentage of keystrokes that could have been saved when only one prediction was shown will be referred to as 'Classical Keystrokes Saved' (CKS).
\item The popular smartphone application SwiftKey always shows the user three predictions, which seem to be (1) what the user has keyed in so far, (2) the most likely prediction and (3) the second most likely prediction. In case the user has not yet started typing the next word, option (1) is replaced by the third most likely prediction. Because smartphones use a touchscreen as an input device, no extra keystrokes are needed. The percentage of keystrokes that could have been saved when two (and sometimes three) predictions were shown will be referred to as 'SwiftKey Keystrokes Saved' (SKKS).
\end{itemize}

\subsection{Procedure}
From the 50 selected Twitters used, the 10\% most recent Tweets were used to test on. For \textbf{experiment 1}, which investigated idiolects, 51 language models were created: 50 based on the remaining on the remaining 90\% for each Twitter user, and 1 based on the written Dutch corpus. Soothsayer calculated the CKS and SKKS for all 50 testfiles twice: one when using the general language model, and one when using the language model based on earlier tweets by the same author.

For \textbf{experiment 2}, which investigated sociolects, another 100 language models were created. The first fifty of these were trained on 90\% of the material from the selected Twitter users \emph{and} the tweets by their friends (sociolect 1). The other fifty language models were also trained 90\% of the material from the selected Twitter users \emph{and} the tweets of the friends of a random other Twitter user (sociolect 2). The same 50 test files used in experiment 1 were used to calculate the CKS and SKKS.

\section{Results}

\begin{table*}[t]
\begin{tabular}{l|*{17}{l}}
Twitter user&0&1&2&3&4&5&6&7&8&9&10&11&12&13&14&15&16\\
\hline
Idiolect CKS\\
Idiolect SKKS\\
\hline
General Model CKS\\
General Model SKKS\\
\hline
Sociolect 1 CKS\\
Sociolect 1 SKKS\\
\hline
Sociolect 2 CKS\\
Sociolect 2 SKKS\\
\end{tabular}
\end{table*}

\begin{table*}[t]
\begin{tabular}{l|*{17}{l}}
Twitter user&17&18&19&20&21&22&23&24&25&26&27&28&29&30&31&32&33\\
\hline
Idiolect CKS\\
Idiolect SKKS\\
\hline
General Model CKS\\
General Model SKKS\\
\hline
Sociolect 1 CKS\\
Sociolect 1 SKKS\\
\hline
Sociolect 2 CKS\\
Sociolect 2 SKKS\\
\end{tabular}
\end{table*}

\begin{table*}[t]
\begin{tabular}{l|*{17}{l}}
Twitter user&34&35&36&37&38&39&40&41&42&43&44&45&46&47&48&49&50\\
\hline
Idiolect CKS\\
Idiolect SKKS\\
\hline
General Model CKS\\
General Model SKKS\\
\hline
Sociolect 1 CKS\\
Sociolect 1 SKKS\\
\hline
Sociolect 2 CKS\\
Sociolect 2 SKKS\\
\end{tabular}
\end{table*}

\section{Conclusion and discussion}

\end{document}