\documentclass[11pt]{article}
\usepackage{eacl2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{covington}
\usepackage{graphicx}
\usepackage{float}
\usepackage{qtree}

\special{papersize=210mm,297mm} % to avoid having to use "-t a4" with dvips 
%\setlength\titlebox{6.5cm}  % You can expand the title box if you really have to

\title{Using idiolects and sociolects to improve word prediction}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\}

\date{}



%Set font
%\renewcommand{\familydefault}{\sfdefault}

%Tables captions small
%\usepackage{caption3} % load caption package kernel first
%\DeclareCaptionOption{parskip}[]{} % disable "parskip" caption option
%\usepackage[footnotesize]{caption}

%Line spacing
%\linespread{1.5}

%New page for every chapter
%\let\stdsection\section
%\renewcommand\section{\newpage\stdsection}

%Center tables
\let\originaltable\table
\let\endoriginaltable\endtable
\renewenvironment{table}[1][ht]{%
  \originaltable[#1]
  \centering}%
  {\endoriginaltable}

\begin{document}

\maketitle

\begin{abstract}
In this paper the word prediction system \emph{Xxx}\footnote{Name of the system is anonymized. The system is available as an interactive demo and its source code is publicly available on GitHub.} is described. This system predicts what a user is going to write as he/she is keying it in. The main innovation of Xxx is that it not only uses \emph{idiolects} as its source of knowledge the language of one individual person, but also \emph{sociolects}, the language of the social circle around that person. We use Twitter for data collection and experimentation. During the spring of 2013, all tweets from a large group of Dutch Twitter users were (automatically) collected. For the 100 most active ones, Xxx then tried to predict the later tweets on the basis of earlier tweets. We then added \emph{sociolect} models. These models are based on the tweets of a particular person \emph{and} the tweets of the people he/she often communicates with. The idea behind is that people who often communicate start to talk alike; in other words, the language of the friends of person $x$ can be helpful in trying to predict what person $x$ is going to say. This approach achieved the best results. For a number of users, more than 50\% of the keystrokes could have been saved if they had used Xxx. 
\end{abstract}

\section{Introduction} \label{intro}

Our main aim of the study presented here is to show that the concepts of \emph{idiolect} and \emph{sociolect}, the language of one person and his or her social circle, can be used to improve \emph{word prediction}, the task of predicting what a user is going to type, as he/she is typing. Word prediction technology reduces the number of keystrokes we have to make, and thus saves us time and effort and prevents us from making mistakes. With the rise of smartphones,  on which one  cannot type as fast as on a normal keyboard, word prediction has become widely known and used. Preceding this popularization, word prediction systems were already developed up to three decades ago to assist people with speech and motor disabilities, like cerebral palsy or hemiplexia. By using a device equipped with word prediction technology, they can increase their communication rate considerably \cite{Garay-Vitoria+06}. Indeed, most studies before the year 2000, when mobile phones were not widely used yet, targeted the disabled user group - Copestake \shortcite{copestake97} even reports building a system for one individual user. More recent work targets a wider audience, but in this paper we return to the idea of using an individual's own language to train individualized models.

The concept of an idiolect, the language of only one person, is well-known, but rarely ever modelled or in some other way operationalized \cite{mollin09,barlow10,louwerse04}. Almost every claim in the field of linguistics concerns language as a whole; whether the subject of investigation is a particular syntactic construction, phonological variable, or some other linguistic phenomenon, the results are always supposed to hold for an entire language variety. According to Mollin \shortcite{mollin09} it is a 'neglected area in corpus linguistics', and Barlow \shortcite{barlow10} states that the term 'is distinguished by the fact that there is probably no other linguistic term in which there is such a large gap between the familiarity of the concept and lack of empirical data on the phenomenon.' This is remarkable, since 'idiolects are the only kind of language we can collect data on', as Haugen \shortcite{Haugen72} points out; a language variety essentially is a collection of idiolects.

Word prediction systems typically operate with an algorithm and a language model, as the overview of related work in Section~\ref{related} will show. Language models are created from training material, typically large collection of text. Section \ref{algorithm} introduces our system and its modules. The resulting best-performing algorithm is used in Section \ref{model}, in which we investigate which language model should be used together with this algorithm. We expand the notion of using an individual's idiolect to also using the language of the people this individual communicates with, in Section~\ref{input_networks}. In Section~\ref{conclusion} we offer our conclusions and formulate points for further research.

\section{Related work}
\label{related}

An early solution for word prediction was to use frequency lists, i.e. word lists with occurrence counts per word. Although it is possible to wait until a word unicity point has been reached (the point in a word where there there is no other word with the same prefix), more keystrokes may be saved if the prediction can be done before the unicity point. One of the first word prediction systems using this technique is PAL \cite{swiffin+85}.  Since then, numerous authors have shown that taking context into account improves prediction accuracy significantly \cite{Lesher+99,Garay-Vitoria+06,Tanaka-Ishii07,vandenbosch+08}. This entails that the system has some information on which words are likely to follow each other, and uses this when giving predictions. A simple approach to implementing context-sensitivity is applying the frequency list technique to word $n$-grams \cite{hunnicutt87}.

Obviously, the accuracy of a context-sensitive system largely depends on how often a similar context was available in the training material; since an algorithm can only know what will follow a particular context if it has seen that context before, we should try to give it as much training material as possible. A key publication by Lesher {\em et al.} \shortcite{Lesher+99} indeed shows that the accuracy of a context-sensitive word prediction system is related to how much training material is provided. On the other hand, once most of the frequent combinations are covered, it takes more and more training material to improve the results a little bit. This means we also expect a loglinear effect of the amount of training material on the results of a word prediction system. Van den Bosch \shortcite{vandenbosch11} shows this is indeed the case: in his system, the step from 100 to 1,000 words in the training material, roughly 6\% more keystrokes could saved (from 15\% to 21\% keystrokes saved), whereas the same is true for the step from 1,000,000 to 10,000,000 words (from 40\% to 46\%).

A large portion of the work on word prediction includes linguistic knowledge in some way. A relatively simple approach is to use part-of-speech tags besides orthographical representations of words \cite{carlberger+97,Fazly+03,copestake97,Matiasek+02,garay-vitoria+97}, where the system is also trained to know what PoS-tags are likely to follow each other, possibly with the additional help of an incremental parser and with knowledge of the morphology of the language. This linguistic information is then used to limit the pool of predicted words. Interestingly, most authors conclude that including linguistic knowledge improves the results, but only slightly \cite{garay-vitoria+97,Fazly+03}. As Hunnicutt \shortcite{hunnicutt87} puts it: 'It appears [the implemented grammar] gives small reward for a great deal of effort if one looks at a measure such as keystroke savings'. Fazly and Hirst \shortcite{Fazly+03} note that adding explicit linguistic knowledge 'might not be considered worth the considerable extra cost that it requires'. In the current study we have not used any explicit linguistic knowledge.

Humans use more than knowledge of language alone to predict what somebody is going to say; we also take into account what the speaker typically says, what the speaker is talking about, in what context. Besides the linguistic context, we may therefore also want to give the system some real-world context. One way may be is to use training material from the same domain. Verberne {\em et al.} \shortcite{verberne+12} show that trying to predict Wikipedia text, tweets, transcriptions of conversational speech and Frequently Asked Questions all worked best when using texts from the same type. Furthermore, they show that questions about neurological issues, for which little training material was available, could best be predicted with the corpus that was closest in terms of topics discussed: the medical pages from Wikipedia. 

Van den Bosch \shortcite{vandenbosch11} proposes to make a word prediction system learn about register and topic on the fly with a \emph{recency buffer}. This buffer stores the $n$ latest words; if a word the user is keying in matches one of the words in the recency buffer, this word is suggested instead of what the system would actually have suggested. The idea behind this is that if the user is writing about, for example, traveling, words like 'go', 'hotel', and 'see' are likely to be in the buffer and thus could be suggested quickly. In other words, the systems learns about the user and the topic on the fly.

Although both approaches help to increase the number of keystrokes saved, they also have downsides: for the system by Verberne {\em et al.} \shortcite{verberne+12} training texts in the same genre are needed, which might not be available, whereas the system by Van den Bosch \shortcite{vandenbosch11} ignores context information that it should weigh more intelligently. For example, while a context-sensitive text-prediction system will probably be able to predict \emph{to} for the sentence \emph{they were going t...}, the one with the recency buffer will predict \emph{they}.


\section{System description} \label{algorithm}

Our system works with a set of independent word prediction modules. Modules can be either context-insensitive or context-sensitive, and use one language model. We will work with two language models, one based on a large collection of texts sampling from many different authors, the 'general' language model', and one based on a set of texts written by an individual, the 'idiolect'. We thus have four possible modules:

\begin{table}[htb]
\begin{tabular}{lll} 
Module&Type&Model\\
\hline
1& {\footnotesize Context-sensitive} & {\footnotesize idiolect}\\
2& {\footnotesize Context-sensitive} & {\footnotesize general language model} \\
3& {\footnotesize Context-insensitive} & {\footnotesize idiolect}\\ 
4& {\footnotesize Context-insensitive} & {\footnotesize general language model} \\
\end{tabular} 
\caption{Four possible modules: combinations of type and language model}
\end{table}

Modules can be concatenated in such a way that a second module takes over once the first modules no longer has predictions, a third module takes over once the second one no longer has predictions, etcetera. This makes it possible to use multiple prediction techniques \emph{and} multiple language models in one prediction system.

\paragraph{Evaluating the system} \label{evaluation}

There is an ongoing discussion in the literature on what is the best way to evaluate word prediction systems. A straightforward evaluation might be to calculate the percentage of correctly predicted words (the so-called \emph{hit-ratio}), but as Garay-Vitoria and Abascal \shortcite{Garay-Vitoria+06} note, this is not not enough: a system that has 100\% of the words correct, but only gives this prediction at the last letter of every word saves very few keystrokes. A more natural way might be to test with real humans, and measure how much time they save when using the system \cite{carlberger+97,koester+98,Garay-Vitoria+06}. However, this is a costly and time-consuming task, as the participants will need a considerable amount of time to get used to the system. Therefore, we will evaluate Xxx by simulating somebody keying in a text, and counting how many keystrokes this virtual user does not have to do when using the system. 

However, even when using simulations, there are multiple ways to evaluate the system. One possibility is to provide the user with a list of the $n$ most likely predictions \cite{Lesher+99,Fazly+03}. This approach has the advantage that it results in high percentages of keystrokes saved - in particular when $n$ is set to a high value, because this means the system can do multiple guesses at once, while only one has to be correct. As Van den Bosch \shortcite{vandenbosch11} notes, however, this also has important downsides: 

\begin{quotation}
[I]n many devices and circumstances it is inefficient or impossible to present [...] suggestion lists. Inspecting a list of suggestions also poses a larger cognitive load than checking a single suggestion, and furthermore it is unclear how scrolling, browsing and selecting items from this list should be counted in terms of keystrokes.
\end{quotation}

For this reason, we calculate the number of keystrokes that could have been saved when the user was presented only one prediction at a time. Predictions can be accepted with the space key. Because this is sometimes problematic (for instance, if the user wanted to type \emph{sun}, but Xxx predicts \emph{sunday}, hitting space would lead to the wrong word), a rejection is also calculated as a keystroke. The number of keystrokes that can be saved if the word prediction system works this way will be called \textbf{Classical Keystrokes Saved} (CKS) in the remainder of this thesis.

On the other hand, current popular smartphone applications suggest this approach might be too strict. The popular smartphone application SwiftKey\footnote{\url{http://www.swiftkey.net/}} always shows the user three predictions, which seem to be (1) what the user has keyed in so far, (2) the most likely prediction and (3) the second most likely prediction. In case the user has not yet started typing the next word, option (1) is replaced by the third most likely prediction. The percentage of keystrokes that can be saved when two (and sometimes three) predictions were shown will be referred to as \textbf{SwiftKey Keystrokes Saved} (SKKS). This percentage will likely be higher than CKS.

\subsection{Context-insensitive modules} \label{ci}

Context-insensitive modules only use information of the word the user is currently keying in. In sentence \ref{only_c}, for example, only the c will be used for prediction. 

\begin{examples}
\item I ate too much c \label{only_c}
\end{examples}

This means that a prediction like \emph{communication} is fully possible, despite the context. This also means that at the beginning of a each new word no prediction will be available, because the module has no material to work with. Despite these limitations, context-insensitive modules can already save a lot of keystrokes, because the first few letters of a word impose strong limitations on what letters can possibly follow, and some words have early unicity points.

The context-insensitive module takes as input the word the user is currently keying in. If, on the basis of what has been given, only one word is still possible, this word will be returned as a prediction. To determine which words are possible, the module uses a lexicon. 
Rather than waiting for each word's unicity point, which may not come with many short words, the module aims at a prediction \emph{before} the word has reached its unicity point. In such a case the module generates multiple guesses of all words that are still possible as completions of the current character sequence. As this may be a large list, a good ranking is necessary if this number exceeds the low number of options that can be shown in an interface. A strong ranking metric is word frequency in a background corpus. Therefore, each time the users keys in a new letter, Xxx will go through an ordered frequency list. The first (and thus most frequent and most likely) word that matches with what has been keyed in so far is given as a prediction.

\subsection{Context-sensitive modules} \label{cs}

Context-sensitive modules make use of the words that came before the current word to limit what words are predicted. Xxx approaches word prediction as a classification task, where the words in the context are the features, and the word following this context is the class label to be predicted. This means that we have a separate class for every word that could possibly be predicted. Xxx uses the $k$-nearest neighbour classification method, which is insensitive to the number of classes to be predicted. $k$-nearest neighbour classification (henceforth \emph{KNN}) means that the class is determined on the basis of similar cases in a training corpus. How many cases are taken into consideration, $k$, can be determined beforehand. The similarity between a new instance and memorized instances is determined using a simularity function.  A classic implementation of \emph{KNN} suited for the type of symbolic features we have, the IB1-algorithm \cite{aha+91}, simply counts how many features overlap. However, the IB1 algorithm generally is too slow to be used in practical applications, ours included. We adopt IGTree\footnote{IGTree is implemented in the TiMBL software package, \url{http://ilk.uvt.nl/timbl}} \cite{daelemans+97}, an approximation of IB1 that does not require a comparison of of the complete context.

IGTree calculates which features contain most information about the class labels using the Information Gain or Gain Ratio metrics, orders the features from most informative to least informative, and compresses the training set in a decision tree. Classification of a new context reduces to making a small number of decisions (a maximum of three, because we have three features), instead of comparing a new context to thousands to millions of contexts. If we ask IGTree to classify unseen input, the algorithm may not come to a unique decision. Xxx asks the algorithm to return everything it considers a possibility at the deepest node it reached while traversing the decision tree. Analogous to the manner in which the context-insensitive module generates frequency-ranked lists of possible completions, IGTree will produce a frequency-ranked list of possible completions it found at this deepest node in the tree. Xxx then accepts the single (or three) top-ranking suggestion(s).

\subsection{Other considerations}

\paragraph{Context-sensitive before context-insensitive}

The context-sensitive module learns about which words in the training texts typically follow each other, and thus is potentially powerful when it comes to the more frequent, fixed combinations of words and words that often occur in each other's context, but is not useful with words that are also frequent, but were not used earlier in this context. The context-insensitive module, on the other hand, can predict any word, as long as it has been used before, but knows nothing about fixed combinations. In other words, the modules complement each other. Based on the fact that context-sensitive modules have been reported as scoring better than context-insensitive modules in direct comparisons in controlled experiments \cite{Lesher+99}, we rank context-sensitive modules before context-insensitive ones in all studies reported here. The context-insensitive module trained on idiolects precedes the final context-insensitive module trained on a general language corpus; this order reflects the order also found in the context-sensitive modules described in more detail in the next section.

\paragraph{Attenuation}

IGTree is fast in classification, but with tens of millions of training instances it becomes too slow for real-time use, where fast typists may reach as many as twenty keystrokes per second. To alleviate this issue we  use a (simplified version of a) solution from the field of syntactical parsing called \emph{attenuation} \cite{eisner96}. All words in the training material that occur less often than a particular threshold are replaced by a dummy value. Replacing all low-frequent words by one dummy value makes the IGTree considerably smaller and thus faster to traverse during classification. In a pilot experiment an attenuation threshold of 3 turned out to be the most desirable: it leads to the largest increase in speed (from 28 classifications per second to 89) without any measurable decrease in prediction accuracy. For this reason, an attenuation threshold of 3 was used throughout the study.

\paragraph{Handling morphology} \label{early}

Some aspects of morphology are inherently problematic for word completion, in particular compounding, inflections, and suffixes. For example, imagine a user has already written sentence \ref{morphology}, and wants to write the word \emph{cookies}:

\begin{examples}
\item I would really like the c \label{morphology}
\end{examples}

If in the training material the word \emph{cookie} was more frequent, Xxx will suggest that instead of \emph{cookies}. Normally, when a prediction is wrong, the algorithm will find out because the user keys in another letter (so the predicted word no longer matches what the user is typing), but that technique will not work here. For words that only differ in their suffix, the point of difference is at the end of the word, when there is nothing left to predict. Even if the correct word is the second most likely prediction, this will not be suggested, because Xxx has no reason to switch prediction.

However, there is a clue Xxx could use: normally, when a prediction is right, the user will accept it, instead of going on writing. He/she might not accept it immediately (typing often goes faster than mentally processing predictions), but once the user has not accepted a prediction for more than two or three keystrokes in a row, it gets more and more likely the user keeps on typing because the prediction is wrong. In that case, the second most likely prediction could be displayed, which in many cases will be the word with the second most likely suffix. We use this \emph{early prediction switching} method throughout our experiments.

\paragraph{Recency} \label{rb}

As Church \shortcite{church02} showed, the probability that a words recurs twice in a short stretch of text is far higher than its  frequency in language would suggests, which is mainly related to the word's topicality. Whereas knowledge about topics could be covered by training and testing within the same coherent set of texts (e.g. all written by a single person), the aforementioned recency buffer by definition uses more recent text (that is, material from the same text), and might this way be able to do more accurate predictions. We implemented a buffer that remembers the $n$ most recent words, and suggests the most recent one that matches with the word that is currently being keyed in. Following Van den Bosch \shortcite{vandenbosch11} we set $n$ to 300. If no word matches, the next module will take over. In our experiments we have tested the insertion of the recency buffer module after the context-sensitive modules and before the context-insensitive modules (cf. Sections \ref{recbuf} and \ref{twitter_idiolects}).


\section{The model: idiolects} \label{model}

Both Verberne {\em et al.} \shortcite{verberne+12} and Van den Bosch \shortcite{vandenbosch11} try to give their system information about the style and the content of what they are trying to predict: whereas Verberne {\em et al.} \shortcite{verberne+12} used training material similar to the test material, Van den Bosch \shortcite{vandenbosch11} made his system learn about the user on the fly. In this paper we explicitly train word completion modules on texts written by individuals; we train on their idiolects, and intend to measure whether this type of training leads to better keystroke savings than training on a large collection of texts. With the Xxx system described in Section~\ref{algorithm} we can test this suggestion empirically by comparing various models in various configurations.

\subsection{Using Twitter feeds as idiolects} \label{twitter_idiolects}

In this experiment the power of idiolects will be investigated by training and testing an array of systems on one hundred different idiolects of individuals. For this, the micro-blogging service Twitter\footnote{\url{http://www.twitter.com}} is used. Twitter is a micro-blogging service where each user can submit status updates known as tweets, which consist of 140 characters or less. Using the Twitter API, all tweets of a manually created seed set of 40 Dutch Twitter users were retrieved from January until June 2013. Retweets, messages these authors did not produce themselves, were excluded. These seed users were the starting point of an iterative expansion of the set of crawled Twitter uses by following mentions of other users (indicated with the syntax \emph{'@username'}). The goal of this expansion was to find as much active Twitter users as possible for the system to follow, and to capture the network of these users (which we will need for later experiments). The set was extended with 2 users every 30 minutes:

\begin{itemize}
\item One user was selected on the basis of the number of tweets the system had already saved referred to them. Thus to find this person, the system made a frequency list of all \emph{@addressee} mentions. The Twitter user that was highest on this list, but was not already being followed, was added. This way, I could be sure the new person communicated a lot with at least one of the persons the system was already following.
\item Another user was selected on the basis of the number of users the system was already following referred to them. Thus to find this person, the system made a frequency list of all \emph{@addressee} mentions, counting multiple references by one person only once. The Twitter user that was highest on this list, but was not already being followed, was added. This way, I could be sure the new person had a lot of connections to the people the system was already following.
\end{itemize}

The system limits itself to Dutch tweets using a conservative Dutch word frequency list containing highly frequent Dutch words that have no counterpart in other languages. Addressees receiving non-Dutch tweets were not followed.

Concerning the relation between number of tweets and Twitter users, many scholars have noticed that it follows a Pareto-distribution \cite{asur+10,rui+12}. That is, a small part of the Twitter users produce a large part of the tweets. Heil and Piskorski \shortcite{heil+09} show that the top 10\% of prolific Twitter users were responsible for more than 90\% of the tweets. Asur and Huberman \shortcite{asur+10} suggests this behavior can be found across various social media; for example, according to them 'the top 15\% of the most prolific editors account for 90\% of Wikipedia's edits'. The Twitter data collected for this research show a similar pattern, visualized in Figure~\ref{lcurve}. The relatively stable growth in the left part of the graph can be explained by the collection method used, which required the Twitter users to be at least a little active. 

\begin{figure}[htb] \centering
\includegraphics[scale=0.6]{zipf_twitter}
\caption{The amount of words by all Twitter users followed by the system.}
\label{lcurve}
\end{figure} 

This distribution means that using all or a random selection of Twitter users is not likely to lead to good results, because for most users not much material is available. Therefore, only data from the 100 Twitter users for which the most material was harvested are used to build the idiolect models. Twitter accounts run by something other than an individual person (such as a company) were excluded manually. The number of words ranged from 61,098 words for the least active user of the 100 users to 167,685 words for the most active user. As a control model, a general language model for Dutch will be used. For this, a random selection of blogs, emails and Wikipedia articles from the SoNaR corpus for written Dutch \cite{oostdijk+13} was made. These texts were chosen because they were believed to be neither very formal nor very informal, and fall in the same new-media category as Twitter messages. The control corpus consisted of 55,212,868 words.

\begin{table*}[htb] 
\centering
\begin{tabular}{ll|llll} 
Training material&Test material&\multicolumn{2}{l}{Without recency buffer}&\multicolumn{2}{l}{With recency buffer}\\
\hline
&&Mean&St. dev.&Mean&St. dev.\\
General&Twitter&14.4&5.1&23.2&5.2\\
Idiolect&Twitter&23.2&7.9&26.7&7.9\\
Idiolect + general &Twitter&26.4&6.2&29.7&6.4\\
\end{tabular} 
\caption{Mean percentage of keystrokes saved (\textbf{CKS}) and standard deviations for all module set-ups.} \label{twitter_results_cks}
\end{table*}

\begin{table*}[htb] 
\centering
\begin{tabular}{ll|llll} 
Training material&Test material&\multicolumn{2}{l}{Without recency buffer}&\multicolumn{2}{l}{With recency buffer}\\
\hline
&&Mean&St. dev.&Mean&St. dev.\\
General&Twitter&16.2&6.1&26&5.4\\
Idiolect&Twitter&24.8&8.3&27.9&7.2\\
Idiolect + general&Twitter&28.2&6.3&32.1&6.3\\
\end{tabular} 
\caption{Mean percentage of keystrokes saved (\textbf{SKKS}) and standard deviations for all module set-ups.} \label{twitter_results_skks}
\end{table*}

First, we compared the general language model against each user's idiolect, and tested on all 100 Twitter feeds of individual users. We then combined the two models (the general model acting as back-off for the idiolect model). These three setups were tested with and without a recency buffer module, resulting in six runs. Tables \ref{twitter_results_cks} and \ref{twitter_results_skks} list the results on these six runs measured in CKS and SKKS, respectively. We observe the following:

\begin{enumerate}
\item Using the idiolect model leads to more keystrokes saved than using the general model;
\item Using the general language model as a background model leads to more keystrokes saved than using the idiolect model alone;
\item Using the recency buffer leads to more keystrokes saved;
\item The effect of the recency buffer is the clearest when it is used in addition to the general model.
\end{enumerate}

An ANOVA for repeated measures showed that there is a significant effect of the training material $F(2,198) = 109.495, p > .001$ and whether the recency buffer was used $F(1,99) = 469.648, p > .001$. Contrast analyses revealed that both the differences between the results of the general model and the idiolect model $F(1,99) = 41.902, p > .001$ and the idiolect model and the idiolect model with the background model $F(1,99) = 232.140, p > .001$ were significant.

The high standard deviations indicate a lot of variation. The substantial individual differences are illustrated in Figure \ref{chaos}, where the users are ordered from least to most material. Contrary to expectations, no correlation between amount of training material and the results could be detected (Pearson's correlation, $p = .763$); apparently, the individual factor is that much stronger, and Xxx performs much better for one than for the other. Using the overall best-performing module set-up, the set-up with the idiolect model, backed up by the general language model, \emph{and} the recency buffer, the worst result is {21.8\% CKS and 24.1\% SKKS for user 90, and the best result is 51.3\% CKS and 52.4\% SKKS for user 97.

\begin{figure}[htb] \centering
\includegraphics[scale=0.6]{twitter_chaos}
\caption{The proportion of keystrokes saved for individual Twitter users, ordered from by amount of tweets (from left to right: from least to most), when using the best-performing module set-up}
\label{chaos}
\end{figure} 


The large amount of variation between individual Twitter users cannot easily be explained. From manual inspections of the Twitter accounts some of the data could to a limited extent be explained (for example, people with powerful idiolect models sometimes often repeated long words like \emph{goedemorgen} 'good morning', \emph{dankjewel} 'thank you', and \emph{welterusten} 'sleep well'), but no clear patterns emerged. Trying to predict for which persons word prediction will go well and for which persons it will not might be an interesting topic for future research. It is a question that is related to the field of computational stylometry and in particular automatic authorship attribution, although authorship attribution is the exact opposite of the task described here (guessing the author on the basis of text instead of guessing the text on the basis of the author) \cite{bagavandas+08}.

\section{Social networks and language input} \label{input_networks}

The findings by Lesher {\em et al.} \shortcite{Lesher+99} suggest that more material leads to more keystrokes saved; this may also hold for idiolects: more idiolectal training material might lead to even better results. This material, however, might not be available, simply because not all people write or tweet that much. For a particular user $x$, what other sources of language do we have that might be similar to the idiolect of $x$?

One of the more obvious answers might be the language of the people $x$ often communicates with. The fact that people that are in some way related to each other speak alike using a 'group language or a sociolect, is well established in sociolinguistics; it is arguably the very basis of the whole field. If we can harness a person's sociolect, we could test the hypothesis that adding the language of the people person $x$ communicates with a lot to his/her language model may improve the prediction accuracy, because these people are likely to have a similar idiolect.

This approach of including the language of the people from a particular person's environment can also be viewed from a different perspective: so far, we have followed Mollin \shortcite{mollin09} and Barlow\shortcite{barlow10} in using only the \emph{output} of speakers. This makes sense (since what comes out must have been inside), but can never be the full story, as there might be much more language in a person's head than the language he/she utters. A more natural alternative to capture somebody's idiolect would be to follow a person from birth, and record all linguistic input he/she gets. The sociolect model that will be constructed here can be seen as a feasible and simple approximation of this: by including the language of the socially related persons of person $x$, the system can have a rough idea of the kind of input person $x$ gets.

As was explained in Section \ref{data_twitter_idiolects}, the Twitter data were collected in a way that captures social relations realistically. This makes the current experiment possible. Sociolects were created by collecting all addressees mentioned with the \emph{@addressee} syntax for each of the 100 Twitter users used in the previous experiment. For all addressees that were mentioned three times or more, it was checked if this addressee was in the dataset (which was almost always the case). If so, it was checked whether this addressee also mentioned the original Twitter user at least three times. If this was also the case, the system assumed the users speak to each other often enough to have their language adjusted to each other, and the tweets of this addressee were added to the sociolect of the original Twitter user. We thus end up with 100 sociolects built around the 100 most active Twitter users, all based on the tweets of a Twitter user and the tweets of the persons that he communicated with at least six times (three times as writer, three times as reader).

The results of Verberne {\em et al.} \shortcite{verberne+12} would predict that adding tweets in general would lead to increases in the number of keystrokes saved, as this is using more texts from the same genre. To be sure that any improvements can be attributed to the fact that this is the language from friends, a control model will be build. While the sociolect model consists of the tweets of Twitter user $x$ and the tweets of the friends of twitter user $x$, the control model consists of the tweets of Twitter user $x$ and the tweets of random other Twitter users, and has approximately the same number of words.

For each of the 100 Twitter users, comparative runs are performed with the model created on the basis of the idiolect and the random Twitter users versus the sociolect model. The best performing module set-up from the previous experiments is used.  The results are compared to the simulations with the idiolect model from the previous experiment. The results of the simulations are summarized in Table \ref{socio_results}.
We observe the following:

\begin{enumerate}
\item Adding more tweets to the idiolects leads to more keystrokes saved;
\item The most keystrokes can be saved when using the tweets of the people the owner of the idiolect communicates with often.
\end{enumerate}


\begin{table*}[htb] 
\centering
\begin{tabular}{ll|llll} 
Training material&Test material&\multicolumn{2}{l}{CKS}&\multicolumn{2}{l}{SKKS}\\
\hline
&&Mean&St. dev.&Mean&St. dev.\\
Idiolect&Twitter feed&29.6&6.4&32.1&6.3\\
Control model&Twitter feed&31.2&6.3&33.9&6\\
Sociolect&Twitter feed&33.9&7.1&36.2&7.1\\
\end{tabular} 
\caption{Mean percentage of keystrokes saved when using an idiolect, a control model (consisting of an idiolect and random other Twitter feeds) and a sociolect.} \label{socio_results}
\end{table*}


An ANOVA for repeated measures showed that there is a significant effect of the training material $F(2,198) = 69.466, p > .001$. Contrast analyses revealed that both the differences between the results of the idiolect model and the idiolect model and random feeds $F(1,99) = 93.471, p > .001$ and the idiolect model and random feeds and the sociolect model $F(1,99) = 61.871, p > .001$ are significant.

\begin{table*}[htb] 
\centering
\begin{tabular}{l|llllll} 
Twitter user&\multicolumn{2}{l}{Idiolect}&\multicolumn{2}{l}{Idiolect+random feeds}&\multicolumn{2}{l}{Sociolect}\\
\hline
&CKS&SKKS&CKS&SKKS&CKS&SKKS\\
24&31.2&36.3&34&36.4&31.6&34.3\\
49&27.2&29.1&26.2&29.7&24.6&27.2\\
71&27.5&30.2&34.2&35.8&30.8&32.9\\
\end{tabular} 
\caption{Percentage of keystrokes saved for 3 individual Twitter users, using the the idiolect, control and sociolect models}
\label{deviations}
\end{table*}

Again, the high standard deviations indicate notable variation among the individual results. Table~\ref{deviations} lists the deviating individual scores for three individual Twitter users. In these results we see an increase when random tweets are added, but a decrease when the tweets from their conversation partners are used. For user 24 and 49, the percentage of keystrokes saved when using the sociolect model is even lower than the idiolect model alone.  

Using the best-performing module set-up in general, the set-up with the sociolect model, backed up by the general language model, \emph{and} the recency buffer, the worst result is 21.3\% CKS and 22\% SKKS for user 90, and the best result is 56.2\% CKS and 58.1\% SKKS for user 38.


\section{Conclusion} \label{conclusion}

In this paper we presented the word prediction system \emph{Xxx}. Testing the system we found that

\begin{enumerate}
\item Word prediction and idiolects are an effective combination;
\item When building a word prediction system, concentrating on either the algorithm or the language model alone is not enough;
\item What works well for one speaker, might not necessarily work for another;
\item The fact that people speak like the people around them can also be useful to word prediction.
\end{enumerate}

From here, there are a number of ways this research could go. One could, for example, try to answer the questions these conclusions inevitably raise. What causes the differences between speakers? Can we, on the basis of texts the user wrote earlier, predict what will work and what will not? Or could we adapt the system on the fly, so that it can be most useful to a particular user? And what would be a practical implication of the discoveries related to sociolects? Would a social media plugin to share language models between friends work? 

Another group of questions that can still be answered can be tested when the system's constraints are changed. What would be the effects of predicting more words at the same time? What are Xxx's pros and cons if we focus on one specific domain of word prediction, such as query completion?

Whatever the answers to these questions may be, they will only improve (or clear up) what we have so far: well-performing, state-of-the-art word prediction software, freely available on the internet for everyone to change, improve, and use. Xxx can be used however you want: as standalone software wrapped inside other software, from the cloud with its HTTP-server mode, or even as a Python module.

%\section{Acknowledgements/dankwoord}

\bibliography{thesisbib}{}
\bibliographystyle{acl}

\end{document}
